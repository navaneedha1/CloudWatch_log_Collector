# Author: Brian Lucero, Greg Eppel
# Date Created: Jun 26 2020
# Purpose: Get CloudWatch CPUUtilization and mem_used_percent EC2 metric data and write to S3 in CSV format. Intent
#          is to use this script to extend the functionality so usage metrics could be included.
#          In the context of that tool, this script would be made into a Lambda in the architecture.
#
# Execution Requirements:
# 1. Underlying Python/AWSCLI environment configured with  an IAM user in the organization's root account. In other
#    words, the script will be run with the organization's root account credentials. This is only because the script was
#    tested/validated that way.
# 2. All child accounts in the organization must have the same role configured to assume. For example, the role
#    'OrganizationTREXAccessRole' was added to the child accounts in the organization used for testing, and that role
#    was assumed in each account to retrieve the metric data. That role had all of the needed permission for
#    CloudWatch and EC2.
# 3. The CloudWatch Agent must be installed on all of the EC2 instances where the metric mem_used_percent is wanted, and
#    the Agent must be configured consistently (e.g. with the same metric namespace) across all instances.
# 3. The target S3 bucket should have a bucket policy that only allows the IAM user in the Org root account running this
#    script to write to it.

# TODO: Format metric data, create a CSV file in the target S3 bucket if it doesn't already exist,
# otherwise, append the data to the CSV.
import io

import boto3
import logging
import pandas as pd
from datetime import datetime, timedelta
from boto.s3.key import Key
import awswrangler as wr

# GetMetricData Information
# You can use the GetMetricData API to retrieve as many as 500 different metrics
# in a single request,with a total of as many as 100,800 data points
metricPeriod = 300
pageSize = 500
endTime = datetime.now()
startTime = datetime.now() - timedelta(days=1)
bucketName = "cwdata101"
regionList = ["us-west-2"]


def processMetricResults(cw_client, metrics):
    metricDataQueries = [];

    metricNum = 0;

    for metric in metrics:
        metricNum = metricNum + 1

        metricDataQueries.append(
            {
                'Id': "m" + str(metricNum),
                'MetricStat': {
                    'Metric': {
                        'Namespace': metric["Namespace"],
                        'MetricName': metric["MetricName"],
                        'Dimensions': metric["Dimensions"],
                    },
                    'Period': metricPeriod,
                    'Stat': 'Average'
                    # TO DO - Need the ability to "overide this stat", based on a Namespace,Dimension,Metric in a "config"
                },
            }
        );




    #print(metricDataQueries)
    # Get the GMD paginator
    gmdPaginator = cw_client.get_paginator('get_metric_data')

    # Get the GMD iterator
    metricDataIterator = gmdPaginator.paginate(
        MetricDataQueries=metricDataQueries,
        StartTime=startTime,
        EndTime=endTime,
        ScanBy="TimestampDescending",
        PaginationConfig={
            'PageSize': pageSize
        },
    )


    # TO DO - Need to add code to get "MetricId" from metricDataQueries so we know the Metric/Dimension headers

    appended_data = []

    for mdrPage in metricDataIterator:

        for mdr in mdrPage["MetricDataResults"]:

            metricId = mdr["Id"]
            label = mdr["Label"]
            timeStamps = mdr["Timestamps"]
            values = mdr["Values"]
            # print(values)
            # print(timeStamps)
            for i, ts in enumerate(timeStamps):
                # print(accountId + "," + region + "," + str(timeStamps[i]) + "," + str(values[i]) + "," + label.replace(
                #   ' ', ','))
                appended_data.append(accountId + "," + region + "," + str(timeStamps[i]) + "," + str(values[i]) + "," + label.replace(
                    ' ', ','))


    print(appended_data)
    df = pd.read_csv(io.StringIO('\n'.join(appended_data)))
    path = f"s3://cwdata101/data6.csv"
    wr.s3.to_csv(df, path, index=False)

    #print(df)
    # # Create a pandas data frame out of the returned metric dictionaries
    # # df1
    # #
    # # Create an S3 client
    # s3_client = boto3.resource('s3')
    # bucket = s3_client.Bucket(bucketName)
    # df = pd.DataFrame()
    # # # # Initialize CSV file name, not ideal, but the intention is to look for another CSV file in the
    # # # # target bucket with the same name then create if it doesn't exit or else append to it
    # csv = datetime.today() + '.csv'
    # #
    # # List objects in bucket and branch flow appropriately to create a CSV or append to a CSV
    # for obj in bucket.objects.all():
    #     if obj.key == csv:
    #          # Append to CSV
    #      else:
    # #         # Create CSV


def collectMetrics(cw_client, region):
    # Initialize an array to store our MetricDataQueries
    metricDataQueries = [];

    # List Metric API (get only active metrics using RecentlyActive property)
    # Can be used optimized the number of calls, no need to retrieve "empty" metrics
    # If you are not clear on what metrics you should retrieve you can use this to list ALL the metrics for a namespace
    paginator = cw_client.get_paginator('list_metrics')

    metrics = [];

    # # CPUUtilization
    #
    # for response in paginator.paginate(Dimensions=[{'Name': 'InstanceId'}],
    #                                     MetricName='CPUUtilization',
    #                                     Namespace='AWS/EC2'):
    #      metrics = metrics + response['Metrics'];
    #
    # # NetworkIn
    # for response in paginator.paginate(Dimensions=[{'Name': 'InstanceId'}],
    #                                    MetricName='NetworkIn',
    #                                    Namespace='AWS/EC2'):
    #     metrics = metrics + response['Metrics'];
    # #
    # # NetworkOut
    # for response in paginator.paginate(Dimensions=[{'Name': 'InstanceId'}],
    #                                    MetricName='NetworkOut',
    #                                    Namespace='AWS/EC2'):
    #     metrics = metrics + response['Metrics'];
    #
    # # All Metrics for EBS Namespace
    # for response in paginator.paginate(Namespace='AWS/EBS'):
    #     metrics = metrics + response['Metrics'];
    #
    # # All Metrics in CWAgent Namespace with DimensionSet (Instance Name, ImageId, InstanceId, InstanceType, objectname)
    # for response in paginator.paginate(Dimensions=[
    #     {'Name': 'Instance Name', 'Name': 'ImageId', 'Name': 'InstanceId', 'Name': 'InstanceType',
    #      'Name': 'objectname'}],
    #                                    Namespace='CWAgent'):
    #     metrics = metrics + response['Metrics'];

    # All Metrics in ContainerInsights Namespace
    for response in paginator.paginate(Namespace='ContainerInsights'):
        metrics = metrics + response['Metrics'];

    #  Metrics in DirectConnect Namespace
    for response in paginator.paginate(Dimensions=[{'Name': 'ConnectionId'}],
                                       MetricName='ConnectionState',
                                       Namespace='AWS/DX'):
        metrics = metrics + response['Metrics'];

     #  Metrics in s3 Namespace
    for response in paginator.paginate(Dimensions=[{'Name': 'BucketName'}],
                                       MetricName='BucketSizeBytes',
                                       Namespace='AWS/S3'):
        metrics = metrics + response['Metrics'];

        #  Metrics in s3 Namespace
    for response in paginator.paginate(Dimensions=[{'Name': 'BucketName'}],MetricName='GetRequests',Namespace='AWS/S3'):

        metrics = metrics + response['Metrics'];

    #  Metrics in DynamoDB AccountProvisionedReadCapacityUtilization Namespace
    for response in paginator.paginate(MetricName='AccountProvisionedReadCapacityUtilization',
                                       Namespace='AWS/DynamoDB'):
        metrics = metrics + response['Metrics'];

    #  Metrics in DynamoDB AccountProvisionedWriteCapacityUtilization Namespace
    for response in paginator.paginate(MetricName='AccountProvisionedWriteCapacityUtilization',Namespace='AWS/DynamoDB'):
        metrics = metrics + response['Metrics'];

    #  Metrics in DynamoDB ConsumedReadCapacityUnits Namespace
    for response in paginator.paginate(Dimensions=[{'Name':'TableName'}],MetricName='ConsumedReadCapacityUnits',Namespace='AWS/DynamoDB'):
        metrics = metrics + response['Metrics'];

    #  Metrics in DynamoDB ConsumedWriteCapacityUnits Namespace
    for response in paginator.paginate(Dimensions=[{'Name':'TableName'}],MetricName='ConsumedWriteCapacityUnits',Namespace='AWS/DynamoDB'):
        metrics = metrics + response['Metrics'];

    #  Metrics for the number of WorkSpaces that have a user connected
    for response in paginator.paginate(Dimensions=[{'Name':'DirectoryId'}],MetricName='UserConnected', Namespace='AWS/WorkSpaces'):
        metrics = metrics + response['Metrics'];

    if (len(metrics) > 500):
        print("GetMetricData has a 500 metric limit, splitting into multiple GMD calls");


    # Process the MetricResults and send to S3
    metricList = []

    for metric in metrics:

        metricList.append(metric)

        if (len(metricList) >= 500):
            processMetricResults(cw_client, metricList)
            metricList = []

    if (len(metricList) > 0):
        processMetricResults(cw_client, metricList)


# Initialize AWS Organizations boto3 client
orgClient = boto3.client('organizations')

# Build list of account IDs in the Org
#accountList = []
#accountDict = orgClient.list_accounts()

#for account in accountDict['Accounts']:
#    accountList.append(account['Id'])

accountList = ["810835092760"]

# For each account
for accountId in accountList:

    # Initialize STS client
    sts_client = boto3.client('sts')

    # Assume appropriate role. The appropriate role can be the administrator role that is configured when a new
    # account is created in an AWS Org (role automatically added to account) or when an account joins an Org by invite
    # (role must be manually added to account), though it is most likely not a security best practice to use that role
    # because it does not follow the principle of least privilege. The point is to make sure whatever role used is the
    # same role across all accounts in the Org and only has the permissions it needs to accomplish its task. In this
    # case, the role is named 'OrganizationTREXAccessRole'.
    #assumed_role_object = sts_client.assume_role(
    #    RoleArn="arn:aws:iam::" + accountId + ":role/OrganizationAccountAccessRole",
    #   RoleSessionName="Session-" + accountId,
    #   DurationSeconds=3600  # 3600s = 1h
    # )

    # Store credentials of the assumed role
    # credentials = assumed_role_object['Credentials']

    print(accountId)  # - here for debugging purposes

    # Initiate CloudWatch client in the same region as the EC2 instances
    cw_client = boto3.client(
        'cloudwatch'
        #aws_access_key_id=credentials['AccessKeyId'],
        #aws_secret_access_key=credentials['SecretAccessKey'],
        # aws_session_token=credentials['SessionToken']
    )
    response = cw_client.list_metrics()


    # For each region in the region list
    for region in regionList:
        cw_client.region_name = region
        collectMetrics(cw_client, region)